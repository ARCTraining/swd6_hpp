{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SWD 6.5 Numba GPU.ipynb","provenance":[],"authorship_tag":"ABX9TyMd4swLDeqjIAT3680xpgW5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Qnx6TyyBRrd4"},"source":["# CUDA programming\n","\n","Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model.\n","\n","One feature that significantly simplifies writing GPU kernels is that Numba makes it appear that the kernel has direct access to NumPy arrays. NumPy arrays that are supplied as arguments to the kernel are transferred between the CPU and the GPU automatically (although this can also be an issue).\n","\n","Numba does not yet implement the full CUDA API, so some features are not available. However the features that are provided are enough to begin experimenting with writing GPU enable kernels. CUDA support in Numba is being actively developed, so eventually most of the features should be available.\n","\n","## Terminology\n","\n","Several important terms in the topic of CUDA programming are listed here:\n","\n","host : the CPU\n","\n","device : the GPU\n","\n","host memory : the system main memory\n","\n","device memory : onboard memory on a GPU card\n","\n","kernel : a GPU function launched by the host and executed on the device\n","\n","device function : a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n","\n","## Device Management\n","\n","Connect to a GPU runtime and list the currently connected GPUs:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CyjcP9M8RkgS","executionInfo":{"status":"ok","timestamp":1623150067991,"user_tz":-60,"elapsed":1271,"user":{"displayName":"Martin Callaghan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisklH0uOUyh5WQyi5z1Iu2Q5UJJzq5b3tc4W99kA=s64","userId":"14343904755675752784"}},"outputId":"a3d226b2-0e62-4213-c7ee-98047cdb3d32"},"source":["from numba import cuda\n","print(cuda.gpus)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["<Managed Device 0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CKMwn1bHSgk5"},"source":["## Writing CUDA kernels\n","\n","CUDA has an execution model unlike the traditional sequential model used for programming CPUs. In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). Your solution will be modeled by defining a thread hierarchy of grid, blocks, and threads.\n","\n","Numba also exposes three kinds of GPU memory:\n","\n","* global device memory\n","* shared memory\n","* local memory\n","\n","For all but the simplest algorithms, it is important that you carefully consider how to use and access memory in order to minimize bandwidth requirements and contention.\n","\n","NVIDIA recommends that programmers focus on following those recommendations to achieve the best performance:\n","\n","* Find ways to parallelise sequential code\n","* Minimise data transfers between the host and the device\n","* Adjust kernel launch configuration to maximize device utilization\n","* Ensure global memory accesses are coalesced\n","* Minimise redundant accesses to global memory whenever possible\n","* Avoid different execution paths within the same warp"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"DI3jbbPfSF0b","executionInfo":{"status":"error","timestamp":1623150133372,"user_tz":-60,"elapsed":353,"user":{"displayName":"Martin Callaghan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisklH0uOUyh5WQyi5z1Iu2Q5UJJzq5b3tc4W99kA=s64","userId":"14343904755675752784"}},"outputId":"1fcacb38-2fb8-499f-c517-97524a5137c6"},"source":[""],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-05a77bf9b2b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'numba' is not defined"]}]},{"cell_type":"code","metadata":{"id":"njKk4y3ZSX1s"},"source":[""],"execution_count":null,"outputs":[]}]}