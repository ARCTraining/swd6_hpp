
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>GPUs &#8212; SWD6: High Performance Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '06_GPUs';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="SWD6: High Performance Python - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="SWD6: High Performance Python - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    SWD6: High Performance Python
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="00_overview.html">Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="01_profiling.html">Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_data_structures_algorithms_libraries.html">Data Structures, Algorithms, and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_vectorisation.html">Vectorisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_compilers.html">Compilers</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="08_summary.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ARCTraining/swd6_hpp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ARCTraining/swd6_hpp/issues/new?title=Issue%20on%20page%20%2F06_GPUs.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/06_GPUs.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>GPUs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numba-for-cuda-gpus">Numba for CUDA GPUs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorize-for-gpus"><code class="docutils literal notranslate"><span class="pre">vectorize</span></code> for GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-cuda-kernels">Custom CUDA kernels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rapids">RAPIDS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cupy">cuPy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostics">Diagnostics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jax">JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-can-replace-numpy-for-gpus">JAX can replace NumPy for GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jit-compiler"><code class="docutils literal notranslate"><span class="pre">@jit</span></code> compiler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-vectorisation-with-vmap">Automatic vectorisation with <code class="docutils literal notranslate"><span class="pre">vmap()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions"><span class="xref std std-ref">Solutions</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-information">Further information</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#good-practises">Good practises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-options">Other options</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gpus">
<h1>GPUs<a class="headerlink" href="#gpus" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/ARCTraining/swd6_hpp/blob/master/docs/06_GPUs.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This GPU lesson focuses primarily on NVIDIA CUDA which is a proprietary solution, and that there are open-source alternatives such as OpenCL.
However, at present CUDA is the most used platform for GPU programming and therefore is included in this course.
<strong>Please note this means the following code will not run on Apple Macs at they do not have compatible hardware</strong></p>
</div>
<div class="tip admonition">
<p class="admonition-title">Tip</p>
<p>If you’re in COLAB or have a local CUDA GPU, you can follow along with this section (i.e., uncomment the GPU code bits).</p>
<p>For those in COLAB, ensure the session is using a GPU by going to: Runtime &gt; Change runtime type &gt; Hardware accelerator = GPU.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs (Graphics Processing Units)</a> are optimised for numerical operations, while <a class="reference external" href="https://en.wikipedia.org/wiki/Central_processing_unit">CPUs (central processing units)</a> perform general computation.</p>
<p>Originally, GPUs handled computer graphics. However, they are now used to do a wide range of computations too. Hence, the term <a class="reference external" href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">General Purpose GPU (GPGPU)</a>.</p>
<p>GPU hardware is designed for data parallelism, where high throughputs are achieved when the GPU is computing the same operations on many different elements at once.</p>
<p>You could use other <a class="reference external" href="https://en.wikipedia.org/wiki/Hardware_acceleration">types of accelerators</a> too, though we’re not going to cover those here.</p>
<section id="numba-for-cuda-gpus">
<h2><a class="reference external" href="http://numba.pydata.org/numba-doc/latest/cuda/index.html">Numba for CUDA GPUs</a><a class="headerlink" href="#numba-for-cuda-gpus" title="Link to this heading">#</a></h2>
<p>Earlier we covered how Numba works on single CPUs with <a class="reference external" href="https://numba.readthedocs.io/en/stable/glossary.html#term-nopython-mode"><code class="docutils literal notranslate"><span class="pre">&#64;njit</span></code></a> and multiple CPUs with <code class="docutils literal notranslate"><span class="pre">parallel</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<p>As a recap:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.0e7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So, for a single CPU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span>
<span class="k">def</span> <span class="nf">my_serial_function_for_cpu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">my_serial_function_for_cpu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>137 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div>
</div>
</div>
</div>
<p>And, for multiple CPUs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_parallel_function_for_cpu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">my_parallel_function_for_cpu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32.6 ms ± 7.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here we used <code class="docutils literal notranslate"><span class="pre">njit</span></code> as this automates the parallelisation process.</p>
<p>This is in contrast to <code class="docutils literal notranslate"><span class="pre">vectorize</span></code> where manual effort is required for parallelisation.</p>
</div>
<section id="vectorize-for-gpus">
<h3><code class="docutils literal notranslate"><span class="pre">vectorize</span></code> for GPUs<a class="headerlink" href="#vectorize-for-gpus" title="Link to this heading">#</a></h3>
<p>Numba also works on <a class="reference external" href="https://developer.nvidia.com/how-to-cuda-python">CUDA</a> GPUs using <a class="reference external" href="https://numba.pydata.org/numba-doc/latest/user/vectorize.html"><code class="docutils literal notranslate"><span class="pre">&#64;vectorize</span></code></a> or <a class="reference external" href="https://numba.readthedocs.io/en/stable/cuda/kernels.html"><code class="docutils literal notranslate"><span class="pre">&#64;cuda.jit</span></code></a>.</p>
<p>This is suitable for bigger data sizes (&gt; 1 MB) and high compute intensities.</p>
<p>This adds additional overhead due to moving data to and from GPUs (<a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda/memory.html">memory management</a>).</p>
<p>Similar to our examples in the compiler lesson, we need to specify the types and target in the signature (i.e., the decorator arguments).</p>
<p>Here, the types are specificed slightly differently i.e., output types(input types).</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Not all NumPy code will work on the GPU. In the following example, we will need to use the <code class="docutils literal notranslate"><span class="pre">math</span></code> library instead.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">float32</span><span class="p">,</span> <span class="n">vectorize</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.0e7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@vectorize</span><span class="p">([</span><span class="s2">&quot;float32(float32)&quot;</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_serial_function_for_gpu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">CudaSupportError</span><span class="g g-Whitespace">                          </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="nd">@vectorize</span><span class="p">([</span><span class="s2">&quot;float32(float32)&quot;</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="k">def</span> <span class="nf">my_serial_function_for_gpu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/np/ufunc/decorators.py:131,</span> in <span class="ni">vectorize.&lt;locals&gt;.wrap</span><span class="nt">(func)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">Vectorize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">**</span><span class="n">kws</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span> <span class="k">for</span> <span class="n">sig</span> <span class="ow">in</span> <span class="n">ftylist</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">131</span>     <span class="n">vec</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">sig</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ftylist</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span>     <span class="n">vec</span><span class="o">.</span><span class="n">disable_compile</span><span class="p">()</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/np/ufunc/deviceufunc.py:391,</span> in <span class="ni">DeviceVectorize.add</span><span class="nt">(self, sig)</span>
<span class="g g-Whitespace">    </span><span class="mi">388</span> <span class="n">funcname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="vm">__name__</span>
<span class="g g-Whitespace">    </span><span class="mi">389</span> <span class="n">kernelsource</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_kernel_source</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_template</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">390</span>                                        <span class="n">devfnsig</span><span class="p">,</span> <span class="n">funcname</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">391</span> <span class="n">corefn</span><span class="p">,</span> <span class="n">return_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_core</span><span class="p">(</span><span class="n">devfnsig</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">392</span> <span class="n">glbl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_globals</span><span class="p">(</span><span class="n">corefn</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">393</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">signature</span><span class="p">(</span><span class="n">types</span><span class="o">.</span><span class="n">void</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="n">a</span><span class="p">[:]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">return_type</span><span class="p">[:]]))</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/vectorizers.py:202,</span> in <span class="ni">CUDAVectorize._compile_core</span><span class="nt">(self, sig)</span>
<span class="g g-Whitespace">    </span><span class="mi">201</span> <span class="k">def</span> <span class="nf">_compile_core</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sig</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">202</span>     <span class="n">cudevfn</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">pyfunc</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>     <span class="k">return</span> <span class="n">cudevfn</span><span class="p">,</span> <span class="n">cudevfn</span><span class="o">.</span><span class="n">overloads</span><span class="p">[</span><span class="n">sig</span><span class="o">.</span><span class="n">args</span><span class="p">]</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">return_type</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/decorators.py:131,</span> in <span class="ni">jit.&lt;locals&gt;._jit</span><span class="nt">(func)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span>     <span class="kn">from</span> <span class="nn">numba.core</span> <span class="kn">import</span> <span class="n">typeinfer</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span>     <span class="k">with</span> <span class="n">typeinfer</span><span class="o">.</span><span class="n">register_dispatcher</span><span class="p">(</span><span class="n">disp</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">131</span>         <span class="n">disp</span><span class="o">.</span><span class="n">compile_device</span><span class="p">(</span><span class="n">argtypes</span><span class="p">,</span> <span class="n">restype</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span>     <span class="n">disp</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">argtypes</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/dispatcher.py:882,</span> in <span class="ni">CUDADispatcher.compile_device</span><span class="nt">(self, args, return_type)</span>
<span class="g g-Whitespace">    </span><span class="mi">875</span> <span class="n">fastmath</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">targetoptions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fastmath&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">877</span> <span class="n">nvvm_options</span> <span class="o">=</span> <span class="p">{</span>
<span class="g g-Whitespace">    </span><span class="mi">878</span>     <span class="s1">&#39;opt&#39;</span><span class="p">:</span> <span class="mi">3</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">targetoptions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;opt&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">879</span>     <span class="s1">&#39;fastmath&#39;</span><span class="p">:</span> <span class="n">fastmath</span>
<span class="g g-Whitespace">    </span><span class="mi">880</span> <span class="p">}</span>
<span class="ne">--&gt; </span><span class="mi">882</span> <span class="n">cc</span> <span class="o">=</span> <span class="n">get_current_device</span><span class="p">()</span><span class="o">.</span><span class="n">compute_capability</span>
<span class="g g-Whitespace">    </span><span class="mi">883</span> <span class="n">cres</span> <span class="o">=</span> <span class="n">compile_cuda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">py_func</span><span class="p">,</span> <span class="n">return_type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">884</span>                     <span class="n">debug</span><span class="o">=</span><span class="n">debug</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">885</span>                     <span class="n">lineinfo</span><span class="o">=</span><span class="n">lineinfo</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">888</span>                     <span class="n">nvvm_options</span><span class="o">=</span><span class="n">nvvm_options</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">889</span>                     <span class="n">cc</span><span class="o">=</span><span class="n">cc</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">890</span> <span class="bp">self</span><span class="o">.</span><span class="n">overloads</span><span class="p">[</span><span class="n">args</span><span class="p">]</span> <span class="o">=</span> <span class="n">cres</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/api.py:443,</span> in <span class="ni">get_current_device</span><span class="nt">()</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span> <span class="k">def</span> <span class="nf">get_current_device</span><span class="p">():</span>
<span class="g g-Whitespace">    </span><span class="mi">442</span>     <span class="s2">&quot;Get current device associated with the current thread&quot;</span>
<span class="ne">--&gt; </span><span class="mi">443</span>     <span class="k">return</span> <span class="n">current_context</span><span class="p">()</span><span class="o">.</span><span class="n">device</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py:220,</span> in <span class="ni">get_context</span><span class="nt">(devnum)</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span> <span class="k">def</span> <span class="nf">get_context</span><span class="p">(</span><span class="n">devnum</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">217</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Get the current device or use a device by device number, and</span>
<span class="g g-Whitespace">    </span><span class="mi">218</span><span class="sd">     return the CUDA context.</span>
<span class="g g-Whitespace">    </span><span class="mi">219</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">220</span>     <span class="k">return</span> <span class="n">_runtime</span><span class="o">.</span><span class="n">get_or_create_context</span><span class="p">(</span><span class="n">devnum</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py:138,</span> in <span class="ni">_Runtime.get_or_create_context</span><span class="nt">(self, devnum)</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span> <span class="n">attached_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_attached_context</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="k">if</span> <span class="n">attached_ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">138</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_or_create_context_uncached</span><span class="p">(</span><span class="n">devnum</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>     <span class="k">return</span> <span class="n">attached_ctx</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py:153,</span> in <span class="ni">_Runtime._get_or_create_context_uncached</span><span class="nt">(self, devnum)</span>
<span class="g g-Whitespace">    </span><span class="mi">147</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;See also ``get_or_create_context(devnum)``.</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span><span class="sd"> This version does not read the cache.</span>
<span class="g g-Whitespace">    </span><span class="mi">149</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">150</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">151</span>     <span class="c1"># Try to get the active context in the CUDA stack or</span>
<span class="g g-Whitespace">    </span><span class="mi">152</span>     <span class="c1"># activate GPU-0 with the primary context</span>
<span class="ne">--&gt; </span><span class="mi">153</span>     <span class="k">with</span> <span class="n">driver</span><span class="o">.</span><span class="n">get_active_context</span><span class="p">()</span> <span class="k">as</span> <span class="n">ac</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">154</span>         <span class="k">if</span> <span class="ow">not</span> <span class="n">ac</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">155</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activate_context_for</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:495,</span> in <span class="ni">_ActiveContext.__enter__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">493</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">494</span>     <span class="n">hctx</span> <span class="o">=</span> <span class="n">drvapi</span><span class="o">.</span><span class="n">cu_context</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">495</span>     <span class="n">driver</span><span class="o">.</span><span class="n">cuCtxGetCurrent</span><span class="p">(</span><span class="n">byref</span><span class="p">(</span><span class="n">hctx</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">496</span>     <span class="n">hctx</span> <span class="o">=</span> <span class="n">hctx</span> <span class="k">if</span> <span class="n">hctx</span><span class="o">.</span><span class="n">value</span> <span class="k">else</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">498</span> <span class="k">if</span> <span class="n">hctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

<span class="nn">File ~/miniconda3/envs/swd6_hpp/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:295,</span> in <span class="ni">Driver.__getattr__</span><span class="nt">(self, fname)</span>
<span class="g g-Whitespace">    </span><span class="mi">292</span> <span class="bp">self</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">294</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialization_error</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">295</span>     <span class="k">raise</span> <span class="n">CudaSupportError</span><span class="p">(</span><span class="s2">&quot;Error at driver init: </span><span class="se">\n</span><span class="si">%s</span><span class="s2">:&quot;</span> <span class="o">%</span>
<span class="g g-Whitespace">    </span><span class="mi">296</span>                            <span class="bp">self</span><span class="o">.</span><span class="n">initialization_error</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">298</span> <span class="k">if</span> <span class="n">USE_NV_BINDING</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">299</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cuda_python_wrap_fn</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>

<span class="ne">CudaSupportError</span>: Error at driver init: 

<span class="n">CUDA</span> <span class="n">driver</span> <span class="n">library</span> <span class="n">cannot</span> <span class="n">be</span> <span class="n">found</span><span class="o">.</span>
<span class="n">If</span> <span class="n">you</span> <span class="n">are</span> <span class="n">sure</span> <span class="n">that</span> <span class="n">a</span> <span class="n">CUDA</span> <span class="n">driver</span> <span class="ow">is</span> <span class="n">installed</span><span class="p">,</span>
<span class="k">try</span> <span class="n">setting</span> <span class="n">environment</span> <span class="n">variable</span> <span class="n">NUMBA_CUDA_DRIVER</span>
<span class="k">with</span> <span class="n">the</span> <span class="n">file</span> <span class="n">path</span> <span class="n">of</span> <span class="n">the</span> <span class="n">CUDA</span> <span class="n">driver</span> <span class="n">shared</span> <span class="n">library</span><span class="o">.</span>
<span class="p">:</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># %%timeit</span>
<span class="c1"># my_serial_function_for_gpu(x)</span>
</pre></div>
</div>
</div>
</div>
<p>Numba also supports generalized ufuncs (covered in the compiler lesson) on the GPU using <a class="reference external" href="http://numba.pydata.org/numba-doc/latest/cuda/ufunc.html#generalized-cuda-ufuncs"><code class="docutils literal notranslate"><span class="pre">guvectorize</span></code></a>.</p>
</section>
<section id="custom-cuda-kernels">
<h3>Custom CUDA kernels<a class="headerlink" href="#custom-cuda-kernels" title="Link to this heading">#</a></h3>
<p>Kernel functions are GPU functions called from CPU code.</p>
<p>Kernels cannot explicitly return a value. Instead, all result data must be written to an array passed to the function (e.g., called <code class="docutils literal notranslate"><span class="pre">out</span></code>). This array can then be transferred back to the CPU.</p>
<p>Kernels work over a grid of threads. This grid needs to be defined in terms of the number of blocks in the grid and the number of threads per block. The indices of this grid are used to add values to the <code class="docutils literal notranslate"><span class="pre">out</span></code> array. The indices can be found using <a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda-reference/kernel.html#numba.cuda.grid"><code class="docutils literal notranslate"><span class="pre">cuda.grid()</span></code></a>.</p>
<p>CUDA kernels are compiled using the <a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda-reference/kernel.html#numba.cuda.jit"><code class="docutils literal notranslate"><span class="pre">numba.cuda.jit</span></code></a> decorator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">numba.cuda.jit</span></code> is different to <code class="docutils literal notranslate"><span class="pre">numba.jit</span></code>, which is for CPUs.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># from numba import cuda</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print(cuda.gpus)</span>
</pre></div>
</div>
</div>
</div>
<p>This should return a message similar to:<br />
&lt;Managed Device 0&gt;.</p>
<p>You can also run the bash command <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> within the IPython cell:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !nvidia-smi</span>
</pre></div>
</div>
</div>
</div>
<p>This returns something like the table below. This shows we have access to a <a class="reference external" href="https://www.nvidia.com/en-gb/data-center/tesla-t4/">NVIDIA Tesla T4 GPU</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Tue<span class="w"> </span>Feb<span class="w"> </span><span class="m">22</span><span class="w"> </span><span class="m">13</span>:59:03<span class="w"> </span><span class="m">2022</span><span class="w">       </span>
+-----------------------------------------------------------------------------+
<span class="p">|</span><span class="w"> </span>NVIDIA-SMI<span class="w"> </span><span class="m">460</span>.32.03<span class="w">    </span>Driver<span class="w"> </span>Version:<span class="w"> </span><span class="m">460</span>.32.03<span class="w">    </span>CUDA<span class="w"> </span>Version:<span class="w"> </span><span class="m">11</span>.2<span class="w">     </span><span class="p">|</span>
<span class="p">|</span>-------------------------------+----------------------+----------------------+
<span class="p">|</span><span class="w"> </span>GPU<span class="w">  </span>Name<span class="w">        </span>Persistence-M<span class="p">|</span><span class="w"> </span>Bus-Id<span class="w">        </span>Disp.A<span class="w"> </span><span class="p">|</span><span class="w"> </span>Volatile<span class="w"> </span>Uncorr.<span class="w"> </span>ECC<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>Fan<span class="w">  </span>Temp<span class="w">  </span>Perf<span class="w">  </span>Pwr:Usage/Cap<span class="p">|</span><span class="w">         </span>Memory-Usage<span class="w"> </span><span class="p">|</span><span class="w"> </span>GPU-Util<span class="w">  </span>Compute<span class="w"> </span>M.<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                               </span><span class="p">|</span><span class="w">                      </span><span class="p">|</span><span class="w">               </span>MIG<span class="w"> </span>M.<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span><span class="p">|</span>
<span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>T4<span class="w">            </span>Off<span class="w">  </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:00:04.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>N/A<span class="w">   </span>66C<span class="w">    </span>P0<span class="w">    </span>30W<span class="w"> </span>/<span class="w">  </span>70W<span class="w"> </span><span class="p">|</span><span class="w">    </span>144MiB<span class="w"> </span>/<span class="w"> </span>15109MiB<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">0</span>%<span class="w">      </span>Default<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                               </span><span class="p">|</span><span class="w">                      </span><span class="p">|</span><span class="w">                  </span>N/A<span class="w"> </span><span class="p">|</span>
+-------------------------------+----------------------+----------------------+
<span class="w">                                                                               </span>
+-----------------------------------------------------------------------------+
<span class="p">|</span><span class="w"> </span>Processes:<span class="w">                                                                  </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span>GPU<span class="w">   </span>GI<span class="w">   </span>CI<span class="w">        </span>PID<span class="w">   </span>Type<span class="w">   </span>Process<span class="w"> </span>name<span class="w">                  </span>GPU<span class="w"> </span>Memory<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">        </span>ID<span class="w">   </span>ID<span class="w">                                                   </span>Usage<span class="w">      </span><span class="p">|</span>
<span class="p">|</span><span class="o">=============================================================================</span><span class="p">|</span>
+-----------------------------------------------------------------------------+
</pre></div>
</div>
<p>So, a simple example to add two numbers together:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @cuda.jit</span>
<span class="c1"># def add_kernel(x, y, out):</span>
<span class="c1">#     index = cuda.grid(1)</span>
<span class="c1">#     out[index] = x[index] + y[index]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s define some input variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># n = 4096</span>
<span class="c1"># x = np.arange(n).astype(np.int32) # [0...4095] on the host (CPU)</span>
<span class="c1"># y = np.ones_like(x)               # [1...1] on the host (CPU)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s move these input variables from the host (CPU) to the device (GPU) for the work:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># x_on_device = cuda.to_device(x)</span>
<span class="c1"># y_on_device = cuda.to_device(y)</span>
<span class="c1"># out_on_device = cuda.device_array_like(x_on_device)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we <a class="reference external" href="https://numba.pydata.org/numba-doc/latest/cuda/kernels.html#choosing-the-block-size">choose the block size</a>, by defining how many blocks are in the grid and how many threads are in each of those blocks.</p>
<p>These two numbers multipled together is the size of the grid (for our 1D example).</p>
<p><img alt="cuda_grid.png" src="_images/cuda_grid.png" /></p>
<p><em><a class="reference external" href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/">Image source</a></em></p>
<p>Some rules of thumb are:</p>
<ul class="simple">
<li><p>Blocks per grid should be a multiple of 32.</p></li>
<li><p>Threads per block should be a multiple of 128.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># blocks_per_grid = 32</span>
<span class="c1"># threads_per_block = 128</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we can call the kernel function.</p>
<p>First, add the grid size arguments.</p>
<p>Then, we pass the input/output variables as arguments to the function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># add_kernel[blocks_per_grid, threads_per_block](x_on_device, y_on_device, out_on_device)</span>
</pre></div>
</div>
</div>
</div>
<p>As these CUDA kernels don’t return a value, we can synchronise the device (GPU) back to the host (CPU) to get the result back.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># cuda.synchronize()</span>
<span class="c1"># print(out_on_device.copy_to_host())</span>
<span class="c1"># # Should be [   1    2    3 ... 4094 4095 4096]</span>
</pre></div>
</div>
</div>
</div>
<p>For more information on CUDA, see the training courses:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arc.leeds.ac.uk/training/courses/hpc5/">HPC5: Introduction to GPU programming with CUDA</a></p></li>
<li><p>NVIDIA workshop on <a class="reference external" href="https://www.nvidia.com/en-us/training/instructor-led-workshops/fundamentals-of-accelerated-computing-with-cuda-python/">Fundamentals of Accelerated Computing with CUDA Python</a></p>
<ul>
<li><p>Detailed look at <a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda/kernels.html">custom CUDA kernels</a> and <a class="reference external" href="https://numba.pydata.org/numba-doc/dev/cuda/memory.html">GPU memory management</a>.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="rapids">
<h2><a class="reference external" href="https://developer.nvidia.com/rapids">RAPIDS</a><a class="headerlink" href="#rapids" title="Link to this heading">#</a></h2>
<p>RAPIDS is a range of accelerated data science libraries from NVIDIA.</p>
<p>There are a wide variety of tools matching up to familiar libraries:</p>
<ul class="simple">
<li><p>Arrays and matrices</p>
<ul>
<li><p><a class="reference external" href="https://cupy.dev/">cuPy</a> for NumPy and SciPy</p></li>
</ul>
</li>
<li><p>Tabular data</p>
<ul>
<li><p><a class="reference external" href="https://docs.rapids.ai/api/cudf/stable/">cuDF</a> for Pandas</p></li>
</ul>
</li>
<li><p>Machine learning</p>
<ul>
<li><p><a class="reference external" href="https://docs.rapids.ai/api/cuml/stable/">cuML</a> for scikit-learn</p></li>
<li><p><a class="reference external" href="https://rapids.ai/xgboost.html">XGBoost</a> on GPUs</p></li>
</ul>
</li>
<li><p>Graphs and networks</p>
<ul>
<li><p><a class="reference external" href="https://docs.rapids.ai/api/cugraph/stable/">cuGraph</a> for <a class="reference external" href="https://networkx.org/">NetworkX</a></p></li>
</ul>
</li>
<li><p>Multiple GPUs</p>
<ul>
<li><p><a class="reference external" href="https://rapids.ai/dask.html">Dask with CUDA</a>, cuDF, cuML, and others.</p></li>
<li><p><a class="reference external" href="http://mpi.dask.org/en/latest/gpu.html">Dask-MPI with GPUs</a></p></li>
</ul>
</li>
</ul>
<section id="cupy">
<h3><a class="reference external" href="https://cupy.dev/">cuPy</a><a class="headerlink" href="#cupy" title="Link to this heading">#</a></h3>
<p><strong>NumPy for the CPU</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1_000</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">)</span>
<span class="n">y_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1_000</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">z_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_cpu</span><span class="p">,</span> <span class="n">y_cpu</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>33.2 ms ± 9.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
<p><strong>CuPy for the GPU</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import cupy as cp</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># x_gpu = cp.random.rand(1_000, 1_000)</span>
<span class="c1"># y_gpu = cp.random.rand(1_000, 1_000)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># %%timeit</span>
<span class="c1"># z_gpu = cp.dot(x_gpu, y_gpu)</span>
</pre></div>
</div>
</div>
</div>
<p>You can move arrays between the CPU and GPU as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># z_gpu = cp.asarray(z_cpu)  # from cpu to gpu</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># z_cpu = cp.asnumpy(z_gpu)  # from gpu to cpu</span>
</pre></div>
</div>
</div>
</div>
<p>For more information on RAPIDS, see the training courses:</p>
<ul class="simple">
<li><p>NVIDIA workshop on <a class="reference external" href="https://www.nvidia.com/en-us/training/instructor-led-workshops/fundamentals-of-accelerated-data-science/">Fundamentals of Accelerated Data Science (RAPIDS)</a>.</p></li>
</ul>
</section>
</section>
<section id="diagnostics">
<h2>Diagnostics<a class="headerlink" href="#diagnostics" title="Link to this heading">#</a></h2>
<p>Similar to the Dask Dashboard, NVIDIA has a GPU Dashboard called <a class="reference external" href="https://github.com/rapidsai/jupyterlab-nvdashboard"><code class="docutils literal notranslate"><span class="pre">NVDashboard</span></code></a>.</p>
<p>These real-time diagnostics are provided via a Bokeh server and a Jupyter Lab extension.</p>
<p>They are a great way to manage your GPU utilisation, resources, throughput, and more.</p>
<p>More information is <a class="reference external" href="https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/">here</a>.</p>
<p><img alt="SegmentLocal" src="_images/NVIDIA_GPUDashboard.gif" /></p>
<p><em><a class="reference external" href="https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/">Image source</a></em></p>
</section>
<section id="jax">
<h2><a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">JAX</a><a class="headerlink" href="#jax" title="Link to this heading">#</a></h2>
<p>JAX enables:</p>
<ul class="simple">
<li><p>NumPy on the CPU and GPU (via <a class="reference external" href="https://www.tensorflow.org/xla">XLA, Accelerated Linear Algebra</a>, a compiler for linear algebra).</p></li>
<li><p>Automatic differentiation of native Python and NumPy code (via <a class="reference external" href="https://github.com/hips/autograd">Autograd</a>).</p></li>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit"><code class="docutils literal notranslate"><span class="pre">Jit</span></code></a> compiler to speed up code.</p></li>
<li><p>Automatic vectorisation through <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap"><code class="docutils literal notranslate"><span class="pre">vmap</span></code></a>.</p></li>
</ul>
<section id="jax-can-replace-numpy-for-gpus">
<h3>JAX can replace NumPy for GPUs<a class="headerlink" href="#jax-can-replace-numpy-for-gpus" title="Link to this heading">#</a></h3>
<p>If it can’t find a GPU, then it will fall back to the CPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x_np</span><span class="p">))</span>
<span class="n">x_np</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_jnp</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">))</span>
<span class="n">x_jnp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-02-25 16:02:15.867936: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-02-25 16:02:15.867960: W external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;jaxlib.xla_extension.DeviceArray&#39;&gt;
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>However, there are some differences between JAX and NumPy.</p>
<p>For example, <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#jax-vs-numpy">JAX arrays are immutable</a> (i.e., you can’t change them once their made).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_np</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x_np</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([10,  1,  2,  3,  4,  5,  6,  7,  8,  9])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">x_jnp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sorry, you can&#39;t change JAX arrays once their made.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sorry, you can&#39;t change JAX arrays once their made.
</pre></div>
</div>
</div>
</div>
<p>Instead, you can create a copy with the change:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">updated_x_jnp</span> <span class="o">=</span> <span class="n">x_jnp</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">updated_x_jnp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([10,  1,  2,  3,  4,  5,  6,  7,  8,  9], dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Also, <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers">random arrays are created differently</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3_000</span><span class="p">,</span> <span class="mi">3_000</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_jnp</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">3_000</span><span class="p">,</span> <span class="mi">3_000</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, you could multiply arrays:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> np.dot(x_np, x_np.T)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>242 ms ± 18.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> jnp.dot(x_jnp, x_jnp.T).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>187 ms ± 1.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div>
</div>
</div>
</div>
<p>By default, this will transfer data from the host (CPU) to the device (GPU).</p>
<p>To avoid this and <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html#jax.device_put">put data</a> onto the device (GPU):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">device_put</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_jnp</span> <span class="o">=</span> <span class="n">device_put</span><span class="p">(</span><span class="n">x_jnp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> jnp.dot(x_jnp, x_jnp.T).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>188 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
<p>In general, the speed comparison between JAX and NumPy is complicated and depends on a variety of things (<a class="reference external" href="https://jax.readthedocs.io/en/latest/faq.html#faq-jax-vs-numpy">read more here</a>).</p>
</section>
<section id="jit-compiler">
<h3><a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit"><code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code></a> compiler<a class="headerlink" href="#jit-compiler" title="Link to this heading">#</a></h3>
<p>Let’s see an example of using the JAX <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> compiler to speed up a function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1_000_000</span><span class="p">,))</span>
</pre></div>
</div>
</div>
</div>
<p>Here were using an example for the <a class="reference external" href="https://arxiv.org/pdf/1706.02515v5.pdf">SELU (Scaled Exponential Linear Unit)</a> activation function (<em>don’t worry what this is</em>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">slow_selu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.67</span><span class="p">,</span> <span class="n">lmbda</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">slow_selu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.12 ms ± 48.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fast_selu</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">slow_selu</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Remember from our compiler lesson, that the first call to a JIT-decorated function compiles it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n 1 -r 1
<span class="n">fast_selu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>25 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
</div>
</div>
<p>Then all subsequent calls to it use the cached, fast version:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it -n 1 -r 1
<span class="n">fast_selu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>489 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</pre></div>
</div>
</div>
</div>
</section>
<section id="automatic-vectorisation-with-vmap">
<h3>Automatic vectorisation with <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap"><code class="docutils literal notranslate"><span class="pre">vmap()</span></code></a><a class="headerlink" href="#automatic-vectorisation-with-vmap" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">vmap</span></code> maps a function over array axes.</p>
<p>This pushes the map loop lower down for better performance.</p>
<p>Let’s see an example of multiplying a matrix by a batch of vectors (<em>don’t worry what this function does, just focus on the JAX bits</em>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">batch_vectors</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">multiplying_matrix_by_vector</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So, first apply this function in a simple batch:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simple_batch</span><span class="p">(</span><span class="n">batch_vectors</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">multiplying_matrix_by_vector</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">batch_vectors</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> simple_batch(batch_vectors).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>726 µs ± 32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>Now, let’s instead use a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> batch to map the function over the matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vmap_batch</span><span class="p">(</span><span class="n">batch_vectors</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">multiplying_matrix_by_vector</span><span class="p">)(</span><span class="n">batch_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> vmap_batch(batch_vectors).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>224 µs ± 19.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>And, let’s get even more performance by combining this with the <code class="docutils literal notranslate"><span class="pre">jit</span></code> compiler:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">faster_vmap_batch</span><span class="p">(</span><span class="n">batch_vectors</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">multiplying_matrix_by_vector</span><span class="p">)(</span><span class="n">batch_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> faster_vmap_batch(batch_vectors).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>23.1 µs ± 1.88 µs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>There is lots more useful information in the <a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">documentation</a>, such as <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax-101/index.html">a range of tutorials</a>.</p>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-1 admonition">
<p class="admonition-title">Exercise 1</p>
<p>In general, what kind of tasks are GPUs faster than CPUs for, and why?</p>
</div>
<div class="admonition-exercise-2 admonition">
<p class="admonition-title">Exercise 2</p>
<p>What Numba decorators can you use to offload a function to GPUs?</p>
</div>
<div class="admonition-exercise-3 admonition">
<p class="admonition-title">Exercise 3</p>
<p>How would you vectorize the the following function for GPUs?</p>
<p><code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">my_serial_function_for_gpu(x):</span></code><br />
<code class="docutils literal notranslate">&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">math.cos(x)</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">+</span> <span class="pre">math.sin(x)</span> <span class="pre">**</span> <span class="pre">2</span></code></p>
</div>
<div class="admonition-exercise-4 admonition">
<p class="admonition-title">Exercise 4</p>
<p>What are ways you can check if your Python environment has access to a GPU?</p>
</div>
<div class="admonition-exercise-5 admonition">
<p class="admonition-title">Exercise 5</p>
<p>If you wanted to do NumPy style work on GPUs, could you use:</p>
<ul class="simple">
<li><p>cuPy</p></li>
<li><p>JAX</p></li>
</ul>
</div>
</section>
<section id="solutions">
<h2><a class="reference internal" href="07_solutions.html#gpus"><span class="std std-ref">Solutions</span></a><a class="headerlink" href="#solutions" title="Link to this heading">#</a></h2>
</section>
<section id="key-points">
<h2>Key Points<a class="headerlink" href="#key-points" title="Link to this heading">#</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="contains-task-list simple">
<li class="task-list-item"><p><input class="task-list-item-checkbox" checked="checked" disabled="disabled" type="checkbox"> <em>Use <a class="reference external" href="https://developer.nvidia.com/how-to-cuda-python">CUDA/Numba</a>, <a class="reference external" href="https://developer.nvidia.com/rapids">RAPIDS</a>, and <a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> to write custom data science code for CUDA GPUs.</em></p></li>
</ul>
</div>
</section>
<section id="further-information">
<h2>Further information<a class="headerlink" href="#further-information" title="Link to this heading">#</a></h2>
<section id="good-practises">
<h3>Good practises<a class="headerlink" href="#good-practises" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Test out ideas on CPUs first, before moving to expensive GPUs.</p></li>
<li><p>Consider whether the calculation is worth the additional overhead of sending data to and from the GPU.</p></li>
<li><p>Minimise data transfers between the host (CPU) and the device (GPU).</p></li>
</ul>
</section>
<section id="other-options">
<h3>Other options<a class="headerlink" href="#other-options" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://documen.tician.de/pycuda/">pycuda</a></p>
<ul>
<li><p>An alternative to Numba for accessing NVIDIA’s CUDA GPUs.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://developer.nvidia.com/cunumeric">cuNumeric</a></p>
<ul>
<li><p>A swap-out replacement for NumPy from NVIDIA for distributed GPUs.</p></li>
<li><p>Early stages of development.</p></li>
<li><p>Requires a separate interperter to run (Legate).</p></li>
</ul>
</li>
<li><p>Many libraries can use GPUs automatically if they can detect one e.g., <a class="reference external" href="https://www.tensorflow.org/install/gpu"><code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html"><code class="docutils literal notranslate"><span class="pre">PyTorch</span></code></a>.</p></li>
</ul>
</section>
<section id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=_AKDqw6li58">CuPy - Sean Farley</a>, PyBay 2019.</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=lV7rtDW94do">cuDF - Mark Harris</a>, PyCon AU 2019.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "swd6_hpp"
        },
        kernelOptions: {
            name: "swd6_hpp",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'swd6_hpp'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numba-for-cuda-gpus">Numba for CUDA GPUs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorize-for-gpus"><code class="docutils literal notranslate"><span class="pre">vectorize</span></code> for GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-cuda-kernels">Custom CUDA kernels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rapids">RAPIDS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cupy">cuPy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostics">Diagnostics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jax">JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-can-replace-numpy-for-gpus">JAX can replace NumPy for GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jit-compiler"><code class="docutils literal notranslate"><span class="pre">@jit</span></code> compiler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-vectorisation-with-vmap">Automatic vectorisation with <code class="docutils literal notranslate"><span class="pre">vmap()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions"><span class="xref std std-ref">Solutions</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-information">Further information</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#good-practises">Good practises</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-options">Other options</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Luke Conibear
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>